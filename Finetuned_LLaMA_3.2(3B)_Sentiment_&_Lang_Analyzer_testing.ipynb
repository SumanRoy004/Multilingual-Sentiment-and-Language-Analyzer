{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install required Packages"
      ],
      "metadata": {
        "id": "vjzufmDcfD0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install langchain_huggingface --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install gradio --quiet\n",
        "!pip install bitsandbytes --quiet\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install huggingface_hub --quiet\n",
        "!pip install huggingface_hub[hf_xet] --quiet"
      ],
      "metadata": {
        "id": "6lmFwFnzH_KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbba2f2-c1ef-426c-b832-7fe64553f5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing required libraries"
      ],
      "metadata": {
        "id": "rLi570eDHz-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import gradio as gr\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "nM459k9OHzHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing HF Token"
      ],
      "metadata": {
        "id": "DlPzgi7YHsl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "huggingface_user=userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "ixdIGE926ffw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "print(whoami(token=huggingface_user))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuan_kvl7AMa",
        "outputId": "e24c6125-8374-44de-97b9-19829a438105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'type': 'user', 'id': '66957888f4d5f5d06ca36462', 'name': 'Suman2004', 'fullname': 'Suman Roy', 'email': 'sumanroy202400@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': 1748735999, 'isPro': False, 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66957888f4d5f5d06ca36462/BQzGRCRMfy4WL6NSsAICJ.jpeg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'sent_token', 'role': 'write', 'createdAt': '2025-05-14T17:59:22.556Z'}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the finetuned model"
      ],
      "metadata": {
        "id": "jIySAzW0I8P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=huggingface_user)\n",
        "model_id = \"Suman2004/lang-trans-sentiment-analyser__finetuned-llama-3.2-3b-instruct-unsloth-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10000\n",
        ")\n",
        "hf = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "TtS1oB5icum1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launching the gradio app"
      ],
      "metadata": {
        "id": "q9WdgH6xNQPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message =''' You are an advanced language model specializing in comprehensive sentiment and language analysis. Your task is to analyze the input text and provide two distinct outputs:\n",
        "  1. Sentiment Analysis:\n",
        "               Identify the sentiment expressed in the text. Categorize the sentiment using the following specific emotion labels:\n",
        "                                               Joy/Happiness\n",
        "                                               Sadness\n",
        "                                               Anger\n",
        "                                               Fear\n",
        "                                               Surprise\n",
        "                                               Disgust\n",
        "                                               Frustration\n",
        "                                               Excitement\n",
        "                                               Anxiety\n",
        "                                               Hope\n",
        "               If the text expresses a mix of emotions, identify the most prominent one.\n",
        "               If the text is neutral or objective, state Sentiment-Neutral.\n",
        "  2. Language Analysis:\n",
        "               Identify the current language of the input and provide translated text into English and original full name of the language of the input.'''"
      ],
      "metadata": {
        "id": "T2xaPhgrkFUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": \"Q: Du ist sehr klug und nett !!\"},]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 30000,\n",
        "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jahwqp3Rja8z",
        "outputId": "8b401332-a031-47f2-f9fd-cedecdf8b44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Sentiment Analysis:\n",
            "   - Detected Sentiment: anger\n",
            "\n",
            "2. Language Analysis:\n",
            "   - Original Language: German\n",
            "   - English Translation: Q: You are very clever and nice!!<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def analyze_text(input_text):\n",
        "    system_message = '''You are an advanced language model specializing in comprehensive sentiment and language analysis. Your task is to analyze the input text and provide two distinct outputs:\n",
        "1. Sentiment Analysis:\n",
        "   Identify the sentiment expressed in the text. Categorize the sentiment using the following specific emotion labels:\n",
        "       Joy/Happiness\n",
        "       Sadness\n",
        "       Anger\n",
        "       Fear\n",
        "       Surprise\n",
        "       Disgust\n",
        "       Frustration\n",
        "       Excitement\n",
        "       Anxiety\n",
        "       Hope\n",
        "   If the text expresses a mix of emotions, identify the most prominent one.\n",
        "   If the text is neutral or objective, state Sentiment-Neutral.\n",
        "\n",
        "2. Language Analysis:\n",
        "   Identify the current language of the input and provide translated text into English and original full name of the language of the input.'''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": input_text},\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    # Generate the response\n",
        "    response = hf.invoke(formatted_prompt)\n",
        "    assistant_response = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].split(\"<|eot_id|>\")[0].strip()\n",
        "    return assistant_response\n",
        "\n",
        "# Gradio interface with an example\n",
        "iface = gr.Interface(\n",
        "    fn=analyze_text,\n",
        "    inputs=gr.Textbox(lines=4, placeholder=\"Enter text here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Multilingual Sentiment and Language Analyzer\",\n",
        "    description=\"Enter text in any language. The model will detect the language, translate it to English, and analyze the sentiment.\",\n",
        "    examples=[\n",
        "        [\"Du ist sehr klug und nett !!\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "A--JX0tCVsN7",
        "outputId": "42c8a83a-2877-4f17-bf19-cebac162635d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b419efd00e34cc8889.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b419efd00e34cc8889.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}